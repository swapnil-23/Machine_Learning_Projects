{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport cv2\nimport os\nimport random\nimport datetime\nimport matplotlib.pyplot as plt\nfrom shapely.wkt import loads as wkt_loads\nimport tifffile as tiff\n\nfrom keras import backend as K\n# from sklearn.metrics import jaccard_similarity_score\nfrom shapely.geometry import MultiPolygon, Polygon\nfrom shapely.wkt import loads\nimport shapely.wkt ## for the manipulation of planar features\nimport shapely.affinity\nfrom collections import defaultdict\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.optimizers import *\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler\nfrom keras import backend as keras\nimport gc\nimport warnings\nimport zipfile\nwarnings.filterwarnings(\"ignore\")\nfrom keras.models import load_model\nimport tensorflow as tf\nimport random as rn\nfrom tensorflow.keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\nfrom tqdm import tqdm ## for the progress meter\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2022-07-07T14:26:30.278104Z","iopub.execute_input":"2022-07-07T14:26:30.279284Z","iopub.status.idle":"2022-07-07T14:26:30.292881Z","shell.execute_reply.started":"2022-07-07T14:26:30.279247Z","shell.execute_reply":"2022-07-07T14:26:30.291856Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## making the directories named data, msk, model_weights, subm","metadata":{}},{"cell_type":"code","source":"os.mkdir('/kaggle/data')\nos.mkdir('/kaggle/msk')\nos.mkdir('/kaggle/model_weights')\nos.mkdir('/kaggle/subm')","metadata":{"execution":{"iopub.status.busy":"2022-07-07T14:26:34.018950Z","iopub.execute_input":"2022-07-07T14:26:34.019919Z","iopub.status.idle":"2022-07-07T14:26:34.027657Z","shell.execute_reply.started":"2022-07-07T14:26:34.019859Z","shell.execute_reply":"2022-07-07T14:26:34.026759Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## making the directories for the training dataset, validation dataset, test dataset","metadata":{}},{"cell_type":"code","source":"os.mkdir('/kaggle/x_tr_a')\nos.mkdir('/kaggle/x_tr_na')\nos.mkdir('/kaggle/y_tr_a')\nos.mkdir('/kaggle/y_tr_na')\n\nos.mkdir('/kaggle/x_val_a')\nos.mkdir('/kaggle/x_val_na')\nos.mkdir('/kaggle/y_val_a')\nos.mkdir('/kaggle/y_val_na')\n\nos.mkdir('/kaggle/x_test_a')\nos.mkdir('/kaggle/x_test_na')\nos.mkdir('/kaggle/y_test_a')\nos.mkdir('/kaggle/y_test_na')","metadata":{"execution":{"iopub.status.busy":"2022-07-07T14:26:36.350849Z","iopub.execute_input":"2022-07-07T14:26:36.351500Z","iopub.status.idle":"2022-07-07T14:26:36.363518Z","shell.execute_reply.started":"2022-07-07T14:26:36.351464Z","shell.execute_reply":"2022-07-07T14:26:36.362608Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Performing EDA","metadata":{}},{"cell_type":"markdown","source":"## reading the train_wkt file for performing the EDA","metadata":{}},{"cell_type":"code","source":"## reading the wkt files\nwkt_df = pd.read_csv('../input/dstl-satellite-imagery-feature-detection/train_wkt_v4.csv.zip')","metadata":{"execution":{"iopub.status.busy":"2022-07-07T14:28:32.149675Z","iopub.execute_input":"2022-07-07T14:28:32.150621Z","iopub.status.idle":"2022-07-07T14:28:32.927607Z","shell.execute_reply.started":"2022-07-07T14:28:32.150581Z","shell.execute_reply":"2022-07-07T14:28:32.926641Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"**Class Label**\n1. Buildings - large building, residential, non-residential, fuel storage facility, fortified building\n2. Misc. Manmade structures\n3. Road\n4. Track - poor/dirt/cart track, footpath/trail\n5. Trees - woodland, hedgerows, groups of trees, standalone trees\n6. Crops - contour ploughing/cropland, grain (wheat) crops, row (potatoes, turnips) crops\n7. Waterway\n8. Standing water\n9. Vehicle Large - large vehicle (e.g. lorry, truck,bus), logistics vehicle\n10. Vehicle Small - small vehicle (car, van), motorbike","metadata":{}},{"cell_type":"markdown","source":"## finding the unique iamges present in the dataset","metadata":{}},{"cell_type":"code","source":"#Unique Images\n\nlen(wkt_df['ImageId'].unique()) \n\n## there are 25 total unqiue image ids","metadata":{"execution":{"iopub.status.busy":"2022-07-07T14:28:51.541824Z","iopub.execute_input":"2022-07-07T14:28:51.542506Z","iopub.status.idle":"2022-07-07T14:28:51.559279Z","shell.execute_reply.started":"2022-07-07T14:28:51.542471Z","shell.execute_reply":"2022-07-07T14:28:51.558257Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## finding the frequency of class label in images","metadata":{}},{"cell_type":"code","source":"wkt_df[wkt_df['MultipolygonWKT']!='MULTIPOLYGON EMPTY']['ClassType'].value_counts()\\\n                      .plot.bar(title = 'frequency of class lable in images')\nplt.xlabel('class label')\nplt.ylabel('frequency')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-07T14:29:14.783254Z","iopub.execute_input":"2022-07-07T14:29:14.784233Z","iopub.status.idle":"2022-07-07T14:29:15.164549Z","shell.execute_reply.started":"2022-07-07T14:29:14.784192Z","shell.execute_reply":"2022-07-07T14:29:15.163691Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## the first function is used to find the patches present in the 3 band images\n## the second function is used to adjust the contrast of the 3 band images\n## the third function is used to read the 3 band images","metadata":{}},{"cell_type":"code","source":"# 3-Band images\ndef stretch2(band, lower_percent=2, higher_percent=98):\n    a = 0 #np.min(band)\n    b = 255  #np.max(band)\n    c = np.percentile(band, lower_percent)\n    d = np.percentile(band, higher_percent)        \n    out = a + (band - c) * (b - a) / (d - c)    \n    out[out<a] = a\n    out[out>b] = b\n    return out\n\ndef adjust_contrast(x):    \n    for i in range(3):\n        x[:,:,i] = stretch2(x[:,:,i])\n    return x.astype(np.uint8) \n\n## reading the 3 band images\nglobal rgb\ndef display_img(ImageId):\n    zip_path = '../input/dstl-satellite-imagery-feature-detection/three_band.zip'\n    rgbfile = '{}_M.tif'.format(ImageId)\n    with zipfile.ZipFile(zip_path) as myzip:\n        files_in_zip = myzip.namelist()\n        for fname in files_in_zip:\n            if fname.endswith(rgbfile):\n                with myzip.open(fname) as myfile:\n                    rgb = tiff.imread(myfile)\n                    rgb = np.rollaxis(rgb, 0, 3)\n                    return rgb\n                x = adjust_contrast(rgb).copy()\n                ax[1].imshow(x,extent=[0, 0.0092, -0.009, 0])","metadata":{"execution":{"iopub.status.busy":"2022-07-07T14:29:19.819931Z","iopub.execute_input":"2022-07-07T14:29:19.820346Z","iopub.status.idle":"2022-07-07T14:29:19.830695Z","shell.execute_reply.started":"2022-07-07T14:29:19.820310Z","shell.execute_reply":"2022-07-07T14:29:19.829613Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":" #storing one multipoligon in pol\npol = loads(wkt_df['MultipolygonWKT'][11])","metadata":{"execution":{"iopub.status.busy":"2022-07-07T14:29:23.100918Z","iopub.execute_input":"2022-07-07T14:29:23.101971Z","iopub.status.idle":"2022-07-07T14:29:23.153891Z","shell.execute_reply.started":"2022-07-07T14:29:23.101927Z","shell.execute_reply":"2022-07-07T14:29:23.152797Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"wkt_df['Multipolygons'] = wkt_df.apply(lambda a: loads(a.MultipolygonWKT), axis=1)\nwkt_df['Num_Multipolygons'] = wkt_df.apply(lambda a: len(a['Multipolygons'].geoms), axis=1) \nobjects_per_image = wkt_df.pivot(index='ClassType', columns='ImageId', values='Num_Multipolygons')","metadata":{"execution":{"iopub.status.busy":"2022-07-07T14:29:25.897971Z","iopub.execute_input":"2022-07-07T14:29:25.898424Z","iopub.status.idle":"2022-07-07T14:29:27.954601Z","shell.execute_reply.started":"2022-07-07T14:29:25.898383Z","shell.execute_reply":"2022-07-07T14:29:27.953575Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#plotting polygons for all the images\n\nc=0\nfor img_id in wkt_df['ImageId'].unique():\n    df = wkt_df[wkt_df['ImageId']==img_id]\n    fig, ax = plt.subplots(1, 2,figsize=(20, 10))  #increase figsize and run again\n    for ele1 in df['Multipolygons']:\n        for ele2 in ele1:\n            x,y = ele2.exterior.xy\n            ax[0].plot(x,y,'red')\n            ax[0].axis('off')\n    display_img(img_id)\n    #ax[0].title(f\"Image {wkt_df['ImageId'][i]}\")\n    ax[0].set_title(f\"Image {img_id}\")\n    ax[1].axis('off')\n    plt.show()\n    c += 1\n    if c==3:\n        break","metadata":{"execution":{"iopub.status.busy":"2022-07-07T14:29:29.692744Z","iopub.execute_input":"2022-07-07T14:29:29.693366Z","iopub.status.idle":"2022-07-07T14:31:49.769612Z","shell.execute_reply.started":"2022-07-07T14:29:29.693280Z","shell.execute_reply":"2022-07-07T14:31:49.768680Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#class labels in dictionary as key and value pair\nclass_label = {\n    1:'Buildings',\n    2:'Misc. Manmade structures',\n    3:'Road',\n    4:'Track',\n    5:'Trees',\n    6:'Crops',\n    7:'Waterway',\n    8:'Standing water',\n    9:'Vehicle Large',\n    10:'Vehicle Small'\n}","metadata":{"execution":{"iopub.status.busy":"2022-07-07T14:31:59.776371Z","iopub.execute_input":"2022-07-07T14:31:59.777435Z","iopub.status.idle":"2022-07-07T14:31:59.783585Z","shell.execute_reply.started":"2022-07-07T14:31:59.777390Z","shell.execute_reply":"2022-07-07T14:31:59.782154Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"Classes = {1: 'Building', 2: 'Structure', 3: 'Road', 4: 'Track', 5: 'Trees', 6: 'Crops', 7: 'Waterway', 8: 'Standing water', 9: 'Truck', 10: 'Car'}\nColors = {1:'#d73027' , 2: '#f46d43', 3: '#4575b4', 4:'#74add1' , 5: '#dfc27d', 6: '#a6dba0', 7: '#1b7837', 8: '#b35806', 9: '0.4',  10: '0.7'}","metadata":{"execution":{"iopub.status.busy":"2022-07-07T14:32:04.288335Z","iopub.execute_input":"2022-07-07T14:32:04.289282Z","iopub.status.idle":"2022-07-07T14:32:04.295847Z","shell.execute_reply.started":"2022-07-07T14:32:04.289243Z","shell.execute_reply":"2022-07-07T14:32:04.294747Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## finding the image statistics by creating the dataframe which contains the number of classes, number of polygons, and total area of image with image_id as we have the Xmax, Ymin of the image","metadata":{}},{"cell_type":"code","source":"def image_stats(image_id):\n    '''\n    creating dataframe which contains \n    classes, number of polygon as counts and total area of image with id image_id\n    '''\n    counts, total_area = {}, {}\n    xmax = GS[GS.ImageId == image_id].Xmax.values[0]\n    ymin = GS[GS.ImageId == image_id].Ymin.values[0]\n    image_area = abs(xmax * ymin) #area of image\n    for cl in Classes:\n        all_poly = wkt_df[wkt_df.ImageId == image_id]\n        poly = all_poly[all_poly.ClassType == cl].MultipolygonWKT\n        poly_list = wkt_loads(poly.values[0])\n        counts[cl] = len(poly_list)  #number of polygons of classtype cl\n\n        if len(poly_list) > 0:\n            #calculating area of all polygon for class cl having number of polygons more than one\n            total_area[cl] = np.sum([poly.area for poly in poly_list])\\\n                             / image_area * 100 \n   \n    df = pd.DataFrame({'Class': Classes, 'Counts': counts,\n                         'TotalArea': total_area})\n    return df ","metadata":{"execution":{"iopub.status.busy":"2022-07-07T14:32:06.452743Z","iopub.execute_input":"2022-07-07T14:32:06.453722Z","iopub.status.idle":"2022-07-07T14:32:06.462693Z","shell.execute_reply.started":"2022-07-07T14:32:06.453675Z","shell.execute_reply":"2022-07-07T14:32:06.461615Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"##printing the the grid sizes of the images\nGS = pd.read_csv('../input/dstl-satellite-imagery-feature-detection/grid_sizes.csv.zip')\nGS = GS.rename(columns={'Unnamed: 0':'ImageId'})\nprint(GS)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T14:32:11.854339Z","iopub.execute_input":"2022-07-07T14:32:11.854691Z","iopub.status.idle":"2022-07-07T14:32:11.872746Z","shell.execute_reply.started":"2022-07-07T14:32:11.854662Z","shell.execute_reply":"2022-07-07T14:32:11.871742Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## We enumerate the image id in stats_list","metadata":{}},{"cell_type":"code","source":"stats_list = []\nImages = sorted(set(wkt_df['ImageId']))\nfor image_no, image_id in enumerate(Images):\n    stat = image_stats(image_id)\n    stat['ImageId'] = image_id\n    stats_list.append(stat)\n        \nstats = pd.concat(stats_list)\npvt_stats = stats.pivot(index = 'Class', columns = 'ImageId', values = 'TotalArea')\npercent_area = np.cumsum(pvt_stats, axis = 0)\n\nclass_r = {}\nfor cl in Classes:\n    class_r[Classes[cl]] = cl","metadata":{"execution":{"iopub.status.busy":"2022-07-07T14:32:15.385255Z","iopub.execute_input":"2022-07-07T14:32:15.386139Z","iopub.status.idle":"2022-07-07T14:32:18.800054Z","shell.execute_reply.started":"2022-07-07T14:32:15.386100Z","shell.execute_reply":"2022-07-07T14:32:18.799062Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#iterating through all class to plot bar plot\nimport seaborn as sns\nfor cl in np.arange(1, 11):\n    class_name = percent_area.index[-cl]\n    class_id = class_r[class_name]\n    ax = sns.barplot(x = percent_area.columns, y = percent_area.loc[class_name],\n                         color = Colors[class_id], label = class_name)\n\nsns.set_context({'figure.figsize': (20, 8)})\nax.legend(loc = 2)\nsns.set_style(\"dark\")\nax.set_xticklabels(percent_area.columns, rotation = 45)\n\nax.set_xlabel('Image ID')\nax.set_ylabel('Total Area')\nplt.title('Areas of the objects in Image')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-07T14:32:25.367392Z","iopub.execute_input":"2022-07-07T14:32:25.367738Z","iopub.status.idle":"2022-07-07T14:32:26.474632Z","shell.execute_reply.started":"2022-07-07T14:32:25.367707Z","shell.execute_reply":"2022-07-07T14:32:26.473744Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"From the above graph we can see that \n1. Trees have highest area covered\n2. After trees crops have highest area covered\n3. Water and vehicles cover the less area","metadata":{}},{"cell_type":"markdown","source":"# Pre-processing","metadata":{}},{"cell_type":"code","source":"## we are having 10 classes\nnum_cls = 10\n\nsize = 160\n\n#reduces and suppresses image noises\nsmooth = 1e-12\ninDir = '../input/dstl-satellite-imagery-feature-detection'\nTR = pd.read_csv(inDir + '/train_wkt_v4.csv.zip')\nGS = pd.read_csv(inDir + '/grid_sizes.csv.zip', names=['ImageId', 'Xmax', 'Ymin'], skiprows=1)\n\n#SF = pd.read_csv('/content/sample_submission.csv')\nGS = GS.rename( columns={'Unnamed: 0':'ImageId'}) #rename 'ImageId'","metadata":{"execution":{"iopub.status.busy":"2022-07-07T14:33:08.999210Z","iopub.execute_input":"2022-07-07T14:33:09.000111Z","iopub.status.idle":"2022-07-07T14:33:09.564144Z","shell.execute_reply.started":"2022-07-07T14:33:09.000074Z","shell.execute_reply":"2022-07-07T14:33:09.563209Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## Adjusting the contrast by creating a function called adjust_contrast where we take the lower band(a) value as 0 and the higher band(b) value as 1","metadata":{}},{"cell_type":"code","source":"## Adjusting the contrast of the image bands\n\ndef adjust_contrast(bands, lower_percent=2, higher_percent=98):\n    \n    out = np.zeros_like(bands).astype(np.float32)\n    n = bands.shape[2]\n    for i in range(n):\n        a = 0  # min(band)\n        b = 1  # max(band)\n        c = np.percentile(bands[:, :, i], lower_percent)\n        d = np.percentile(bands[:, :, i], higher_percent)\n        t = a + (bands[:, :, i] - c) * (b - a) / (d - c)\n        t[t < a] = a\n        t[t > b] = b\n        out[:, :, i] = t\n\n    return out.astype(np.float32)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T14:33:19.867628Z","iopub.execute_input":"2022-07-07T14:33:19.868697Z","iopub.status.idle":"2022-07-07T14:33:19.877701Z","shell.execute_reply.started":"2022-07-07T14:33:19.868656Z","shell.execute_reply":"2022-07-07T14:33:19.876724Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## We create a function called coordinate to raster(pixels) which take the parameters as coordinates, size of the image(img_size), Xmax and Ymin of the image\n\n","metadata":{}},{"cell_type":"code","source":"## converting the coordinates into raster(pixels)\ndef coordi_to_raster(coords, img_size, xmax, ymax):\n    \n    H, W = img_size\n    W1 = 1.0 * W * W / (W + 1)\n    H1 = 1.0 * H * H / (H + 1)\n    xf = W1 / xmax\n    yf = H1 / ymax\n    coords[:, 1] *= yf\n    coords[:, 0] *= xf\n    coords_int = np.round(coords).astype(np.int32)\n    return coords_int","metadata":{"execution":{"iopub.status.busy":"2022-07-07T14:33:30.215246Z","iopub.execute_input":"2022-07-07T14:33:30.215676Z","iopub.status.idle":"2022-07-07T14:33:30.226182Z","shell.execute_reply.started":"2022-07-07T14:33:30.215640Z","shell.execute_reply":"2022-07-07T14:33:30.225087Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## We create a function called convert_contours which is used to create image masks with multipolygon objects using exterior and interior coordinates, the perim_list is used to store the exterior coordinates and the interior coordinates are stores in the array called interior_list","metadata":{}},{"cell_type":"code","source":"## creating the image masks with mutliploygon objects using exterior and interior coordinates of the given multiploygon\ndef convert_contours(polygonList, raster_img_size, xmax, ymax):\n    \n    perim_list = []\n    interior_list = []\n    if polygonList is None:\n        return None\n    for k in range(len(polygonList)):\n        poly = polygonList[k]\n        perim = np.array(list(poly.exterior.coords))\n        perim_c = coordi_to_raster(perim, raster_img_size, xmax, ymax)\n        perim_list.append(perim_c)\n        for pi in poly.interiors:\n            interior = np.array(list(pi.coords))\n            interior_c = coordi_to_raster(interior, raster_img_size, xmax, ymax)\n            interior_list.append(interior_c)\n    return perim_list, interior_list\n","metadata":{"execution":{"iopub.status.busy":"2022-07-07T14:33:33.864800Z","iopub.execute_input":"2022-07-07T14:33:33.865976Z","iopub.status.idle":"2022-07-07T14:33:33.874081Z","shell.execute_reply.started":"2022-07-07T14:33:33.865881Z","shell.execute_reply":"2022-07-07T14:33:33.872790Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## The function generate_mask_for_image_and_class uses the parameters raster_size, image_id, class_type to create the image masks by creating the polygons using cv2.fillPoly","metadata":{}},{"cell_type":"code","source":"# to generate the image masks using the image_size, image_id, class_type\n\ndef generate_mask_for_image_and_class(raster_size, image_id, class_type):\n\n    xmax, ymax = GS[GS.ImageId == image_id].iloc[0, 1:].astype(float)\n\n    df_image = TR[TR.ImageId == image_id]\n    multipoly_def = df_image[df_image.ClassType == class_type].MultipolygonWKT\n    polygonList = None\n    if len(multipoly_def) > 0:\n        assert len(multipoly_def) == 1\n        polygonList = wkt_loads(multipoly_def.values[0])\n    \n    contours = convert_contours(polygonList, raster_size, xmax, ymax)\n\n    img_mask = np.zeros(raster_size, np.uint8)\n    if contours is None:\n        return img_mask\n    perim_list, interior_list = contours\n    cv2.fillPoly(img_mask, perim_list, 1)\n    cv2.fillPoly(img_mask, interior_list, 0)\n\n    return img_mask","metadata":{"execution":{"iopub.status.busy":"2022-07-07T14:33:42.669304Z","iopub.execute_input":"2022-07-07T14:33:42.669662Z","iopub.status.idle":"2022-07-07T14:33:42.678474Z","shell.execute_reply.started":"2022-07-07T14:33:42.669629Z","shell.execute_reply":"2022-07-07T14:33:42.677522Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"## returns image patches(crops) of given image and mask patch_size = 160*160 pixels\n    \ndef get_patches(img, msk, name1, name2, name3, name4, amt, aug=True):\n    random.seed(42)\n    is2 = int(1.0 * size)\n\n    xm, ym = img.shape[0] - is2, img.shape[1] - is2\n\n    a, b , c, d = [], [], [], []\n\n    # thresholds for each class to get patches\n    tr = [0.4, 0.1, 0.1, 0.15, 0.3, 0.95, 0.1, 0.05, 0.001, 0.005]\n    \n    xyz = np.ceil(amt*0.10).astype(int)\n    amt1 = amt-xyz\n    amt2 = xyz\n\n   # to get augmented data\n    for i in range(amt1):\n\n        xc = random.randint(0, xm)\n        yc = random.randint(0, ym)\n\n        im = img[xc:xc + is2, yc:yc + is2]\n        ms = msk[xc:xc + is2, yc:yc + is2]\n\n     \n        for j in range(num_cls):\n            sm = np.sum(ms[:, :, j])\n\n            if 1.0 * sm / is2 ** 2 > tr[j]:\n               \n                #augmentation\n                if aug:\n                    \n                    # reversing\n                    if random.uniform(0, 1) > 0.5:\n                        im = im[::-1]\n                        ms = ms[::-1]\n\n                    #flipping \n                    if random.uniform(0, 1) > 0.5:\n                        im = im[:, ::-1]\n                        ms = ms[:, ::-1]\n                    rotation = np.random.randint(4) # 0, 1, 2, 3\n\n                    #transpose & rotation\n                    if random.uniform(0, 1) > 0.5:\n                       im = np.rot90(im.transpose((1,0,2)), k=rotation)\n                       ms = np.rot90(ms.transpose((1,0,2)), k=rotation)\n                    \n                    #rotation\n                    if random.uniform(0, 1) > 0.5:\n                      im = np.rot90(im, k=rotation)\n                      ms = np.rot90(ms, k=rotation)\n                    \n                    #shearing \n                    if random.uniform(0, 1) > 0.5:\n                       im = tf.keras.preprocessing.image.apply_affine_transform(im, shear=0)\n                       im = tf.keras.preprocessing.image.apply_affine_transform(im, shear=0)\n                                     \n                \n                im = im.astype(np.float16)\n                ms = ms.astype(np.float16)\n                \n                \n                np.save(\"/kaggle/{}/{}\".format(name1, i),im)  \n                np.save(\"/kaggle/{}/{}\".format(name2, i),ms)  \n               \n                a.append(\"/kaggle/{}/{}.npy\".format(name1, i))\n                b.append(\"/kaggle/{}/{}.npy\".format(name2, i))\n\n    # to get non-augmented data\n    for i in range(amt2):\n        xc = random.randint(0, xm)\n        yc = random.randint(0, ym)\n\n        im = img[xc:xc + is2, yc:yc + is2]\n        ms = msk[xc:xc + is2, yc:yc + is2]\n\n        im = im.astype(np.float16)\n        ms = ms.astype(np.float16)\n                                  \n        np.save(\"/kaggle/{}/{}\".format(name3, i),im)  \n        np.save(\"/kaggle/{}/{}\".format(name4, i),ms)  \n                \n        c.append(\"/kaggle/{}/{}.npy\".format(name3, i))\n        d.append(\"/kaggle/{}/{}.npy\".format(name4, i))\n\n    \n    print(len(a), len(b))\n    print(len(c), len(d))\n  \n    return a+c, b+d","metadata":{"execution":{"iopub.status.busy":"2022-07-07T14:33:59.363011Z","iopub.execute_input":"2022-07-07T14:33:59.363418Z","iopub.status.idle":"2022-07-07T14:33:59.383814Z","shell.execute_reply.started":"2022-07-07T14:33:59.363385Z","shell.execute_reply":"2022-07-07T14:33:59.382509Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"class Dataloder(tf.keras.utils.Sequence):    \n    def __init__(self, dataset, batch_size=1, shuffle=False):\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.indexes = np.arange(len(dataset))\n\n    def __getitem__(self, i):\n        \n        # collect batch data\n        start = i * self.batch_size\n        stop = (i + 1) * self.batch_size\n        data = []\n        for j in range(start, stop):\n            data.append(self.dataset[j])\n        \n        batch = [np.stack(samples, axis=0) for samples in zip(*data)]\n        \n        #print(len(batch))\n        return tuple(batch)\n    \n    def __len__(self):\n        return len(self.indexes) // self.batch_size\n\nclass Dataset:\n  \n    def __init__(self, images_dir, mask_dir):\n        \n        self.ids = images_dir\n        self.images_fps = images_dir\n        self.masks_fps  = mask_dir\n    \n    def __getitem__(self, i):\n        \n        # read data\n        image = np.load(self.images_fps[i]) \n        mask  = np.load(self.masks_fps[i])\n\n          \n        image = np.stack(image, axis=-1).astype('float')\n        mask = np.stack(mask, axis=-1).astype('float')\n\n        #image = np.transpose(image, (1,0,2)) \n        #mask = np.transpose(mask, (1,0,2)) \n    \n        image = np.transpose(image, (0,2,1)) \n        mask = np.transpose(mask, (0,2,1)) \n  \n        return image, mask\n      \n    def __len__(self):\n        return len(self.ids)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T14:34:05.320593Z","iopub.execute_input":"2022-07-07T14:34:05.320968Z","iopub.status.idle":"2022-07-07T14:34:05.334929Z","shell.execute_reply.started":"2022-07-07T14:34:05.320935Z","shell.execute_reply":"2022-07-07T14:34:05.333872Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"## In the following code we perform a rgb detection of 16 band GeoTiff files using M band","metadata":{}},{"cell_type":"code","source":"## rgb image detection using M band\ndef M(image_id):\n    zip_path = '../input/dstl-satellite-imagery-feature-detection/sixteen_band.zip'\n    tgtImg = '{}_M.tif'.format(image_id)\n    with zipfile.ZipFile(zip_path) as myzip:\n        files_in_zip = myzip.namelist()\n        for fname in files_in_zip:\n            if fname.endswith(tgtImg):\n                with myzip.open(fname) as myfile:\n                    img = tiff.imread(myfile)\n                    img = np.rollaxis(img, 0, 3)\n                    return img\n                \n                \n\nprint (\"combining all the images\")\ns = 835\n\nX = np.zeros((5 * s, 5 * s, 8))\nY = np.zeros((5 * s, 5 * s, num_cls))  \n\nids = sorted(set(TR.ImageId))\nprint (len(ids))\n\nfor i in range(5):\n    for j in range(5):\n        id = ids[5 * i + j]\n\n        rgb_img = M(id)\n        img = adjust_contrast(rgb_img).copy()\n        \n        \n        print (img.shape, id)\n        X[s * i:s * i + s, s * j:s * j + s, :] = img[:s, :s, :]\n        for z in range(num_cls):\n            Y[s * i:s * i + s, s * j:s * j + s, z] = generate_mask_for_image_and_class(\n                (img.shape[0], img.shape[1]), id, z + 1)[:s, :s]\n\nnp.save('/kaggle/data/X', X)\nnp.save('/kaggle/data/Y', Y)\nprint(X.shape)\nprint(Y.shape)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T14:34:08.976590Z","iopub.execute_input":"2022-07-07T14:34:08.976975Z","iopub.status.idle":"2022-07-07T14:34:34.345938Z","shell.execute_reply.started":"2022-07-07T14:34:08.976944Z","shell.execute_reply":"2022-07-07T14:34:34.344768Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"markdown","source":"**Splitting the dataset into training, validation and test dataset**","metadata":{}},{"cell_type":"code","source":"#img, msk, name1, name2, name3, name4, amt, aug=True\n#training the dataset\nx_train, y_train = get_patches(X, Y, 'x_tr_a', 'y_tr_a', 'x_tr_na', 'y_tr_na', 20000, aug=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T14:36:47.942865Z","iopub.execute_input":"2022-07-07T14:36:47.943858Z","iopub.status.idle":"2022-07-07T14:38:39.713175Z","shell.execute_reply.started":"2022-07-07T14:36:47.943796Z","shell.execute_reply":"2022-07-07T14:38:39.712080Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"#validation dataset\nx_val, y_val = get_patches(X, Y, 'x_val_a', 'y_val_a', 'x_val_na', 'y_val_na', 4000, aug=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T14:40:07.401726Z","iopub.execute_input":"2022-07-07T14:40:07.402687Z","iopub.status.idle":"2022-07-07T14:40:26.920707Z","shell.execute_reply.started":"2022-07-07T14:40:07.402637Z","shell.execute_reply":"2022-07-07T14:40:26.903464Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"#test dataset\nx_test, y_test = get_patches(X, Y, 'x_test_a', 'y_test_na', 'x_test_a', 'y_test_na', 4000, aug=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T14:40:56.522874Z","iopub.execute_input":"2022-07-07T14:40:56.523213Z","iopub.status.idle":"2022-07-07T14:41:15.821847Z","shell.execute_reply.started":"2022-07-07T14:40:56.523184Z","shell.execute_reply":"2022-07-07T14:41:15.816315Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"train_dataset = Dataset(x_train, y_train)\ntrain_dataloader = Dataloder(train_dataset, batch_size=8)\nval_dataset = Dataset(x_val, y_val)\nval_dataloader = Dataloder(val_dataset, batch_size=8)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T14:41:29.011230Z","iopub.execute_input":"2022-07-07T14:41:29.012411Z","iopub.status.idle":"2022-07-07T14:41:29.018200Z","shell.execute_reply.started":"2022-07-07T14:41:29.012347Z","shell.execute_reply":"2022-07-07T14:41:29.017237Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"train_dataloader[0][0].shape","metadata":{"execution":{"iopub.status.busy":"2022-07-07T14:41:31.909542Z","iopub.execute_input":"2022-07-07T14:41:31.909924Z","iopub.status.idle":"2022-07-07T14:41:32.035303Z","shell.execute_reply.started":"2022-07-07T14:41:31.909892Z","shell.execute_reply":"2022-07-07T14:41:32.034329Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"## We define a function to perform the jaccard_coefficient  where the parameters are y_true, y_pred. The Jaccard Coefficent is used to compare the members for two sets to see which memebers are shared and which are distinct\n\n##   J(x,y) = |x∩y| / |x∪y|  where J(x,y) is the Jaccard Coefficent for x and y","metadata":{}},{"cell_type":"code","source":"## calculating the jaccard coefficient\ndef jaccard_coef(y_true, y_pred):\n    \n    intersection = K.sum(y_true * y_pred, axis=[0, -1, -2])\n    total = K.sum(y_true + y_pred, axis=[0, -1, -2])\n    union = total - intersection\n\n    jac = (intersection + smooth) / (union+ smooth)\n\n    return K.mean(jac)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T14:41:34.194648Z","iopub.execute_input":"2022-07-07T14:41:34.195111Z","iopub.status.idle":"2022-07-07T14:41:34.207611Z","shell.execute_reply.started":"2022-07-07T14:41:34.195069Z","shell.execute_reply":"2022-07-07T14:41:34.206748Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"## We are using SegNet model for the satellite image segmentation the U-net model is able to localise and distinguish borders is by doing classification on every pixel, so the input and output share the same size.","metadata":{}},{"cell_type":"code","source":"##fixing numpy RS\nnp.random.seed(42)\n\n##fixing tensorflow RS\ntf.random.set_seed(32)\n\n##python RS\nrn.seed(12)\n\n## making the SegNet model \n\ndef SegNet():\n    \n    tf.random.set_seed(32)\n    classes= 10\n    \n    ## the input_img is taken as the size of the image patches and the batch size is taken as 8\n    img_input = Input(shape=(size, size, 8))\n    x = img_input\n\n    # Making the Encoder \n    \n    x = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer = tf.keras.initializers.he_normal(seed= 23))(x)\n    x = BatchNormalization()(x)\n    x = Conv2D(64, (3, 3), activation='relu', padding='same',  kernel_initializer = tf.keras.initializers.he_normal(seed= 43))(x)\n   # x = BatchNormalization()(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n    x = Dropout(0.25)(x)\n    \n    x = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer = tf.keras.initializers.he_normal(seed= 32))(x)\n    x = BatchNormalization()(x)\n    x = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer = tf.keras.initializers.he_normal(seed= 41))(x)\n   # x = BatchNormalization()(x)\n    x = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer = tf.keras.initializers.he_normal(seed= 33))(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n    x = Dropout(0.5)(x)\n\n    x = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer = tf.keras.initializers.he_normal(seed= 35))(x)\n    x = BatchNormalization()(x)\n    x = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer = tf.keras.initializers.he_normal(seed= 54))(x)\n    x = BatchNormalization()(x)\n    x = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer = tf.keras.initializers.he_normal(seed= 39))(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n    \n    #Making the Decoder\n    \n    x = UpSampling2D(size=(2, 2))(x)\n    x = Conv2D(128, kernel_size=3, activation='relu', padding='same', kernel_initializer = tf.keras.initializers.he_normal(seed= 45))(x)\n   # x = BatchNormalization()(x)\n    x = Conv2D(128, kernel_size=3, activation='relu', padding='same', kernel_initializer = tf.keras.initializers.he_normal(seed= 41))(x)\n    x = BatchNormalization()(x)\n    x = Conv2D(128, kernel_size=3, activation='relu', padding='same', kernel_initializer = tf.keras.initializers.he_normal(seed= 49))(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.25)(x)\n      \n    x = UpSampling2D(size=(2, 2))(x)\n    x = Conv2D(64, kernel_size=3, activation='relu', padding='same', kernel_initializer = tf.keras.initializers.he_normal(seed= 18))(x)\n    x = BatchNormalization()(x)\n    x = Conv2D(64, kernel_size=3, activation='relu', padding='same', kernel_initializer = tf.keras.initializers.he_normal(seed= 21))(x)\n    x = BatchNormalization()(x)\n    x = Conv2D(classes, kernel_size=3, activation='relu', padding='same', kernel_initializer = tf.keras.initializers.he_normal(seed= 16))(x)\n    x = Dropout(0.25)(x)\n  \n    x = Activation(\"softmax\")(x)\n    \n    model = Model(img_input, x)\n  \n    model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=[jaccard_coef, 'accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-07-07T14:41:37.415787Z","iopub.execute_input":"2022-07-07T14:41:37.416315Z","iopub.status.idle":"2022-07-07T14:41:37.438970Z","shell.execute_reply.started":"2022-07-07T14:41:37.416282Z","shell.execute_reply":"2022-07-07T14:41:37.437879Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"##changing the learning rates\ndef changeLearningRate(epoch):\n\n    #lr=0.001\n    lr=0.0001\n    if epoch > 10 and epoch <=20:\n      lr*=0.1\n    elif epoch > 20 and epoch <=30:\n      lr*=0.01\n    elif epoch > 30 and epoch <=40:\n      lr*=0.001\n    elif epoch > 40 and epoch <=50:  \n      lr*=0.0001\n    elif epoch > 50 and epoch <=60:  \n      lr*=0.0001  \n    elif epoch > 60:\n      lr*=0.0001\n\n    return lr","metadata":{"execution":{"iopub.status.busy":"2022-07-07T14:41:41.040868Z","iopub.execute_input":"2022-07-07T14:41:41.041508Z","iopub.status.idle":"2022-07-07T14:41:41.050697Z","shell.execute_reply.started":"2022-07-07T14:41:41.041471Z","shell.execute_reply":"2022-07-07T14:41:41.049905Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"## We make a threshold function 'myCallback' which is used to stop the neural network.\n\n## If the jaccard coefficent on the validation dataset and the jacard coefficient on the training dataset have value greater than the Accuracy_threshold value that is 0.502 then we stop the training of our deep learning model","metadata":{}},{"cell_type":"code","source":"## to stop the nueral network using callback functions\n\nACCURACY_THRESHOLD=0.502\n\nclass myCallback(tf.keras.callbacks.Callback): \n    \n    def on_epoch_end(self, epoch, logs={}): \n        if (logs.get('val_jaccard_coef') > ACCURACY_THRESHOLD) and (logs.get('jaccard_coef') > ACCURACY_THRESHOLD):   \n          print(\"\\nReached %2.2f%% accuracy, so stopping training!!\" %(ACCURACY_THRESHOLD*100))   \n          self.model.stop_training = True\n\nstop = myCallback()","metadata":{"execution":{"iopub.status.busy":"2022-07-07T14:41:45.204564Z","iopub.execute_input":"2022-07-07T14:41:45.205028Z","iopub.status.idle":"2022-07-07T14:41:45.218465Z","shell.execute_reply.started":"2022-07-07T14:41:45.204986Z","shell.execute_reply":"2022-07-07T14:41:45.217060Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"filepath=\"/kaggle/model_weights/weights-{epoch:02d}-{val_jaccard_coef:.4f}.hdf5\"\n\ncheckpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss',  verbose=1, save_best_only=True, mode='auto')","metadata":{"execution":{"iopub.status.busy":"2022-07-07T14:41:49.121469Z","iopub.execute_input":"2022-07-07T14:41:49.121816Z","iopub.status.idle":"2022-07-07T14:41:49.127106Z","shell.execute_reply.started":"2022-07-07T14:41:49.121786Z","shell.execute_reply":"2022-07-07T14:41:49.126120Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"## We perform the callback function called ReduceLROnPlateau where the learning rate is reduced in order to benefit the model by reducing the learning rate when the metric obtained has stopped improvoing from the current state.","metadata":{}},{"cell_type":"code","source":"## to reduce the learning rate when the metric has stopped improving\n\nrlrop = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose = 1, min_delta = 0.0001)\nlrschedule = LearningRateScheduler(changeLearningRate, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T14:41:52.535622Z","iopub.execute_input":"2022-07-07T14:41:52.535991Z","iopub.status.idle":"2022-07-07T14:41:52.541918Z","shell.execute_reply.started":"2022-07-07T14:41:52.535961Z","shell.execute_reply":"2022-07-07T14:41:52.540702Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"model = SegNet()","metadata":{"execution":{"iopub.status.busy":"2022-07-07T14:41:56.966586Z","iopub.execute_input":"2022-07-07T14:41:56.967138Z","iopub.status.idle":"2022-07-07T14:41:59.881409Z","shell.execute_reply.started":"2022-07-07T14:41:56.967084Z","shell.execute_reply":"2022-07-07T14:41:59.880451Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"## summary of the U-NET model\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-07-07T14:42:53.989731Z","iopub.execute_input":"2022-07-07T14:42:53.990427Z","iopub.status.idle":"2022-07-07T14:42:54.000198Z","shell.execute_reply.started":"2022-07-07T14:42:53.990392Z","shell.execute_reply":"2022-07-07T14:42:53.999140Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"## SegNet Plot summary\n\ntf.keras.utils.plot_model(model, \"model.png\", show_shapes=False, show_dtype=False, show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T14:42:58.990331Z","iopub.execute_input":"2022-07-07T14:42:58.990960Z","iopub.status.idle":"2022-07-07T14:43:00.097870Z","shell.execute_reply.started":"2022-07-07T14:42:58.990924Z","shell.execute_reply":"2022-07-07T14:43:00.096781Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"##fitting the model with 30 epochs\n\nmodel_fitting = model.fit_generator(train_dataloader, \n                              steps_per_epoch=len(train_dataloader),\n                              epochs=30,\n                              validation_data=val_dataloader,\n                              verbose=1,\n                              callbacks=checkpoint\n                              )","metadata":{"execution":{"iopub.status.busy":"2022-07-07T14:43:07.419718Z","iopub.execute_input":"2022-07-07T14:43:07.420503Z","iopub.status.idle":"2022-07-07T15:42:52.726797Z","shell.execute_reply.started":"2022-07-07T14:43:07.420465Z","shell.execute_reply":"2022-07-07T15:42:52.724273Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"**We can see that the accuracy achieved after running the SegNet model is 66.66%**","metadata":{}},{"cell_type":"code","source":"##plotting the training accuracy\nplt.figure(figsize=(15,5))\nplt.plot(range(model_fitting.epoch[-1]+1),model_fitting.history['val_jaccard_coef'],label='val_jaccard_coef')\nplt.plot(range(model_fitting.epoch[-1]+1),model_fitting.history['jaccard_coef'],label='trn_jaccard_coef')\nplt.title('Training Accuracy'); plt.xlabel('Epoch'); plt.ylabel('jaccard_coef');plt.legend(); \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-07T15:43:05.917864Z","iopub.execute_input":"2022-07-07T15:43:05.918906Z","iopub.status.idle":"2022-07-07T15:43:06.228580Z","shell.execute_reply.started":"2022-07-07T15:43:05.918855Z","shell.execute_reply":"2022-07-07T15:43:06.227663Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"test_dataset = Dataset(x_test, y_test)\ntest_dataloader = Dataloder(test_dataset, batch_size=1)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T15:43:11.758374Z","iopub.execute_input":"2022-07-07T15:43:11.758727Z","iopub.status.idle":"2022-07-07T15:43:11.764370Z","shell.execute_reply.started":"2022-07-07T15:43:11.758697Z","shell.execute_reply":"2022-07-07T15:43:11.763364Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"## calculating the score\nScore= []\nfor i in tqdm(range(len(test_dataloader))):\n   pred_msk = model.predict(test_dataloader[i][0])\n   score = jaccard_coef(test_dataloader[i][1], pred_msk)\n   Score.append(score)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T15:43:17.863113Z","iopub.execute_input":"2022-07-07T15:43:17.863977Z","iopub.status.idle":"2022-07-07T15:45:09.045069Z","shell.execute_reply.started":"2022-07-07T15:43:17.863930Z","shell.execute_reply":"2022-07-07T15:45:09.043840Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"score = sum(Score)/len(test_dataloader)\nprint(\"The score obtained on the test data is: \", score.numpy())","metadata":{"execution":{"iopub.status.busy":"2022-07-07T15:45:14.005262Z","iopub.execute_input":"2022-07-07T15:45:14.005655Z","iopub.status.idle":"2022-07-07T15:45:14.060549Z","shell.execute_reply.started":"2022-07-07T15:45:14.005620Z","shell.execute_reply":"2022-07-07T15:45:14.059521Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"# Performing Error Analysis","metadata":{}},{"cell_type":"code","source":"## total x and y values\ntotal_x = x_train + x_val + x_test\ntotal_y = y_train + y_val+ y_test","metadata":{"execution":{"iopub.status.busy":"2022-07-07T15:45:19.699028Z","iopub.execute_input":"2022-07-07T15:45:19.699727Z","iopub.status.idle":"2022-07-07T15:45:19.705087Z","shell.execute_reply.started":"2022-07-07T15:45:19.699688Z","shell.execute_reply":"2022-07-07T15:45:19.703968Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"total_dataset = Dataset(total_x, total_y)\ntotal_dataloader = Dataloder(total_dataset, batch_size=1)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T15:45:22.151923Z","iopub.execute_input":"2022-07-07T15:45:22.152441Z","iopub.status.idle":"2022-07-07T15:45:22.157669Z","shell.execute_reply.started":"2022-07-07T15:45:22.152406Z","shell.execute_reply":"2022-07-07T15:45:22.156528Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"## to compute the similarity between two objects using jaccard coefficient\n\nScore= []\nvery_low_jaccard=[]\nmedium_jaccard= []\nvery_high_jaccard= []\n\nfor i in tqdm(range(len(total_dataloader))):\n\n   pred_msk = model.predict(total_dataloader[i][0])\n   score = jaccard_coef(total_dataloader[i][1], pred_msk)\n   \n   if score>0 and score <=0.20:\n      very_low_jaccard.append(i)\n\n   elif score>0.20 and score <=0.70:\n      medium_jaccard.append(i)\n   \n   elif score>0.70 and score <=1:\n      very_high_jaccard.append(i)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T15:45:24.670185Z","iopub.execute_input":"2022-07-07T15:45:24.670566Z","iopub.status.idle":"2022-07-07T15:58:38.891859Z","shell.execute_reply.started":"2022-07-07T15:45:24.670534Z","shell.execute_reply":"2022-07-07T15:58:38.889333Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"Very_low_jaccard_x = []\nMedium_jaccard_x = []\nVery_high_jaccard_x = []\n\nVery_low_jaccard_y = []\nMedium_jaccard_y = []\nVery_high_jaccard_y = []\n\nfor i in very_low_jaccard:\n   Very_low_jaccard_x.append(total_x[i])\nfor i in medium_jaccard:\n   Medium_jaccard_x.append(total_x[i])\nfor i in very_high_jaccard:\n   Very_high_jaccard_x.append(total_x[i])      \n\nfor i in very_low_jaccard:\n   Very_low_jaccard_y.append(total_y[i])\nfor i in medium_jaccard:\n   Medium_jaccard_y.append(total_y[i])\nfor i in very_high_jaccard:\n   Very_high_jaccard_y.append(total_y[i])","metadata":{"execution":{"iopub.status.busy":"2022-07-07T15:59:33.794916Z","iopub.execute_input":"2022-07-07T15:59:33.796137Z","iopub.status.idle":"2022-07-07T15:59:33.812938Z","shell.execute_reply.started":"2022-07-07T15:59:33.796095Z","shell.execute_reply":"2022-07-07T15:59:33.811876Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"np.save(\"vljx\", Very_low_jaccard_x)\nnp.save(\"vljy\", Very_low_jaccard_y)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T15:59:37.265404Z","iopub.execute_input":"2022-07-07T15:59:37.266045Z","iopub.status.idle":"2022-07-07T15:59:37.282567Z","shell.execute_reply.started":"2022-07-07T15:59:37.266004Z","shell.execute_reply":"2022-07-07T15:59:37.281617Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"vljx = np.load(\"vljx.npy\")\nvljy = np.load(\"vljy.npy\")","metadata":{"execution":{"iopub.status.busy":"2022-07-07T15:59:40.574150Z","iopub.execute_input":"2022-07-07T15:59:40.574512Z","iopub.status.idle":"2022-07-07T15:59:40.581370Z","shell.execute_reply.started":"2022-07-07T15:59:40.574482Z","shell.execute_reply":"2022-07-07T15:59:40.580349Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"## The function mask_to_polygon is used to detect the objects in the image.\n## The openCV function cv2.findContours is used to for extracting the countours from the image.\n## The openCV function cv2.apporxPolyDP is used for approximating the shape of contour of a given polygon to the shape of the original polygon to the specified precision","metadata":{}},{"cell_type":"code","source":"## converting a mask image into polygons\n    \ndef mask_to_polygons(mask, epsilon=5, min_area=1.):\n    \n    ##to detect objects in image\n    contours, hierarchy = cv2.findContours(((mask == 1) * 255).astype(np.uint8), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_TC89_KCOS)\n    approx_contours = [cv2.approxPolyDP(cnt, epsilon, True)\n                       for cnt in contours]\n    if not contours:\n        return MultiPolygon()\n\n    cnt_children = defaultdict(list)\n    child_contours = set()\n    assert hierarchy.shape[0] == 1\n\n    for idx, (_, _, _, parent_idx) in enumerate(hierarchy[0]):\n        if parent_idx != -1:\n            child_contours.add(idx)\n            cnt_children[parent_idx].append(approx_contours[idx])\n\n    # creating actual polygons filtering it by area \n    all_polygons = []\n    for idx, cnt in enumerate(approx_contours):\n        if idx not in child_contours and cv2.contourArea(cnt) >= min_area:\n            assert cnt.shape[1] == 1\n            poly = Polygon(\n                shell=cnt[:, 0, :],\n                holes=[c[:, 0, :] for c in cnt_children.get(idx, [])\n                       if cv2.contourArea(c) >= min_area])\n            all_polygons.append(poly)\n            \n    # approximating polygons might have created invalid ones, fix them\n    \n    all_polygons = MultiPolygon(all_polygons)\n    if not all_polygons.is_valid:\n        all_polygons = all_polygons.buffer(0)\n        # Sometimes buffer() converts a simple Multipolygon to just a Polygon,\n        # need to keep it a Multi throughout\n        if all_polygons.type == 'Polygon':\n            all_polygons = MultiPolygon([all_polygons])\n    return all_polygons","metadata":{"execution":{"iopub.status.busy":"2022-07-07T15:59:43.979632Z","iopub.execute_input":"2022-07-07T15:59:43.982581Z","iopub.status.idle":"2022-07-07T15:59:43.997036Z","shell.execute_reply.started":"2022-07-07T15:59:43.982531Z","shell.execute_reply":"2022-07-07T15:59:43.995784Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"DF  = pd.DataFrame(columns=[\"image\", \"class\", \"poly\"])\n\nfor i in range(25):\n   abcd = np.load(vljy[i])\n   image, cl , ploy = [],[],[]\n  \n   for j in range(10):\n     ab = mask_to_polygons(abcd[:,:,j], epsilon=1)\n     image.append(i+1)\n     cl.append(j+1)\n     ploy.append(len(ab))\n     df = pd.DataFrame(list(zip(image, cl, ploy)), columns = ['image', 'class', 'poly'])\n\n   DF = pd.concat([DF,df], ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T15:59:51.112207Z","iopub.execute_input":"2022-07-07T15:59:51.112744Z","iopub.status.idle":"2022-07-07T15:59:51.932756Z","shell.execute_reply.started":"2022-07-07T15:59:51.112711Z","shell.execute_reply":"2022-07-07T15:59:51.931796Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"objects_per_image = DF.pivot(index='class', columns='image', values='poly')","metadata":{"execution":{"iopub.status.busy":"2022-07-07T15:59:55.779187Z","iopub.execute_input":"2022-07-07T15:59:55.780309Z","iopub.status.idle":"2022-07-07T15:59:55.812081Z","shell.execute_reply.started":"2022-07-07T15:59:55.780262Z","shell.execute_reply":"2022-07-07T15:59:55.810978Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"# OBSERVATIONS","metadata":{}},{"cell_type":"code","source":"print(\"minimum value in an image\",np.amin(np.load(vljx[0])))\nprint(\"maximum value in an image\",np.amax(np.load(vljx[0])))","metadata":{"execution":{"iopub.status.busy":"2022-07-07T15:59:58.575421Z","iopub.execute_input":"2022-07-07T15:59:58.577160Z","iopub.status.idle":"2022-07-07T15:59:58.600695Z","shell.execute_reply.started":"2022-07-07T15:59:58.577108Z","shell.execute_reply":"2022-07-07T15:59:58.599584Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"threshold = 0.4\nSum = []\n\nfor i in tqdm(range(25)):\n   a = np.load(vljx[i])\n   im= []\n   for j in range(8):\n     im.append(np.count_nonzero(np.less(a[:,:,j], threshold))) \n   x = sum(im)\n   Sum.append(x)  \npercentage = (sum(Sum)/(160*160*8*25))*100","metadata":{"execution":{"iopub.status.busy":"2022-07-07T16:00:01.896316Z","iopub.execute_input":"2022-07-07T16:00:01.896728Z","iopub.status.idle":"2022-07-07T16:00:02.093182Z","shell.execute_reply.started":"2022-07-07T16:00:01.896698Z","shell.execute_reply":"2022-07-07T16:00:02.091987Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"print(\"percenatge of values less than threshold 0.4 is\", percentage)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T16:00:05.599146Z","iopub.execute_input":"2022-07-07T16:00:05.599558Z","iopub.status.idle":"2022-07-07T16:00:05.608475Z","shell.execute_reply.started":"2022-07-07T16:00:05.599523Z","shell.execute_reply":"2022-07-07T16:00:05.606973Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"## the depicted percentage above shows that the more values less than taken threshold value implies that image brightness is not so good.","metadata":{}},{"cell_type":"code","source":"## plotting the image\ndef plot_image(image_id):\n\n  m = np.load(vljx[image_id])\n  m = adjust_contrast(m)\n  img = np.zeros((m.shape[0],m.shape[1],3))\n  img[:,:,0] = m[:,:,4] #red\n  img[:,:,1] = m[:,:,2] #green\n  img[:,:,2] = m[:,:,1] #blue\n  #plt.figure(figsize=(7,7))\n  plt.imshow(img, interpolation='nearest')\n  plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-07T16:00:15.639349Z","iopub.execute_input":"2022-07-07T16:00:15.639725Z","iopub.status.idle":"2022-07-07T16:00:15.646365Z","shell.execute_reply.started":"2022-07-07T16:00:15.639693Z","shell.execute_reply":"2022-07-07T16:00:15.645314Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"## plotting the mask image\ndef plot_mask(mask_id):\n  m = np.load(vljy[i])\n  m = adjust_contrast(m)\n  img = np.zeros((m.shape[0],m.shape[1],3)) \n  img[:,:,0] = m[:,:,4] #red\n  img[:,:,1] = m[:,:,2] #green\n  img[:,:,2] = m[:,:,1] #blue\n  #plt.figure(figsize=(7,7))\n  plt.imshow(img, interpolation='nearest')\n  plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-07T16:00:21.468383Z","iopub.execute_input":"2022-07-07T16:00:21.470941Z","iopub.status.idle":"2022-07-07T16:00:21.478211Z","shell.execute_reply.started":"2022-07-07T16:00:21.470902Z","shell.execute_reply":"2022-07-07T16:00:21.476900Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"## printing the final output images\nfor i in range(10):\n   plot_image(i)\n   plot_mask(i)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T16:00:27.472442Z","iopub.execute_input":"2022-07-07T16:00:27.473047Z","iopub.status.idle":"2022-07-07T16:00:32.785894Z","shell.execute_reply.started":"2022-07-07T16:00:27.473012Z","shell.execute_reply":"2022-07-07T16:00:32.784807Z"},"trusted":true},"execution_count":60,"outputs":[]}]}